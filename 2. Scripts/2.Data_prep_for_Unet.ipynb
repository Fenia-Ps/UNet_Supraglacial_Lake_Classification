{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Created on April 2024\n",
        "\n",
        "Author: Fenia Psomouli\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "S-gbvYiXWETS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D4XnCHX27hnf"
      },
      "outputs": [],
      "source": [
        "#Import libraries\n",
        "import os\n",
        "import tempfile\n",
        "import numpy as np\n",
        "import rasterio\n",
        "import pickle\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.colors import Normalize\n",
        "from matplotlib import colors\n",
        "import matplotlib.colors as clr\n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Cloud authentication\n",
        "from google.colab import auth\n",
        "auth.authenticate_user()"
      ],
      "metadata": {
        "id": "V7FVmogrQcUe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Mount Google Drive to Google Colab\n",
        "from google.colab import drive\n",
        "if not os.path.exists('/content/drive'):\n",
        "  drive.mount('/content/drive', force_remount=True)"
      ],
      "metadata": {
        "id": "jfLsqn9nRDZM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Loading the list with the paths for the normalized data for both draining and refreezing lakes\n",
        "draining_paths = np.load('/content/drive/MyDrive/Thesis/Data/file_pathsD64_normalized.npy', allow_pickle=True)\n",
        "refreezing_paths = np.load('/content/drive/MyDrive/Thesis/Data/file_pathsR64_normalized.npy', allow_pickle=True)\n",
        "# Combine draining and refreezing paths into one list, # [:92] -> draining [92:] -> refreezing (93)\n",
        "all_paths = np.concatenate((draining_paths, refreezing_paths)).tolist()"
      ],
      "metadata": {
        "id": "wSoY1iyagK0-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Preparing Antarctic dataset for the prediction\n",
        "antarctica = np.load('/content/drive/MyDrive/Thesis/Data/file_pathsAntarctica_normalized.npy', allow_pickle=True)\n",
        "print(len(antarctica))"
      ],
      "metadata": {
        "id": "Cxtfcb5SsZbj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def _bytes_feature(value):\n",
        "    \"\"\"Returns a bytes_list from a string / byte.\"\"\"\n",
        "    return tf.train.Feature(bytes_list=tf.train.BytesList(value=[tf.io.serialize_tensor(value).numpy()]))\n",
        "\n",
        "def _float_feature(value):\n",
        "    \"\"\"Returns a float_list from a float / double.\"\"\"\n",
        "    return tf.train.Feature(float_list=tf.train.FloatList(value=[value]))\n",
        "\n",
        "def serialize_example(feature, lat, lon):\n",
        "    feature_dict = {\n",
        "        'feature': _bytes_feature(tf.convert_to_tensor(feature, dtype=tf.float32)),\n",
        "        'lat': _float_feature(lat),\n",
        "        'lon': _float_feature(lon)\n",
        "    }\n",
        "    example_proto = tf.train.Example(features=tf.train.Features(feature=feature_dict))\n",
        "    return example_proto.SerializeToString()\n",
        "\n",
        "def read_tiff_file(file_path):\n",
        "    with rasterio.open(file_path) as src:\n",
        "        data = src.read()  # Read all bands at once\n",
        "        features = np.transpose(data[:30, :, :], axes=(1, 2, 0))  # Transpose to height x width x channels\n",
        "        lat, lon = src.xy(0, 0)  # Extract upper-left corner coordinates\n",
        "        return features.astype(np.float32), lat, lon\n",
        "\n",
        "def write_tfrecord(file_name, file_paths):\n",
        "    with tf.io.TFRecordWriter(file_name) as writer:\n",
        "        for file_path in file_paths:\n",
        "            features, lat, lon = read_tiff_file(file_path)\n",
        "            example = serialize_example(features, lat, lon)\n",
        "            writer.write(example)\n",
        "        print(f\"Wrote {len(file_paths)} entries to {file_name}\")\n",
        "\n",
        "# Defining paths\n",
        "file_paths = antarctica\n",
        "\n",
        "# Write data to a TFRecord file\n",
        "tfrecord_filename = '/content/drive/MyDrive/Results/Dataset_Antarctica/antarctica_dataset_with_coords.tfrecords'\n",
        "write_tfrecord(tfrecord_filename, file_paths)\n",
        "\n",
        "def _parse_function(proto):\n",
        "    feature_description = {\n",
        "        'feature': tf.io.FixedLenFeature([], tf.string),\n",
        "        'lat': tf.io.FixedLenFeature([], tf.float32),\n",
        "        'lon': tf.io.FixedLenFeature([], tf.float32)\n",
        "    }\n",
        "    parsed_features = tf.io.parse_single_example(proto, feature_description)\n",
        "    feature = tf.io.parse_tensor(parsed_features['feature'], out_type=tf.float32)\n",
        "    lat = parsed_features['lat']\n",
        "    lon = parsed_features['lon']\n",
        "    feature.set_shape([64, 64, 30])\n",
        "    return feature, lat, lon\n",
        "\n",
        "def load_dataset(tfrecord_file, batch_size):\n",
        "    dataset = tf.data.TFRecordDataset(tfrecord_file)\n",
        "    dataset = dataset.map(_parse_function)\n",
        "    dataset = dataset.batch(batch_size)\n",
        "    return dataset\n",
        "\n",
        "batch_size = 8"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4ejtT5HsWQVl",
        "outputId": "2a73352a-13ad-4f95-8542-6330f3af8ca6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Wrote 1607 entries to /content/drive/MyDrive/Results/Dataset_Antarctica/antarctica_dataset_with_coords.tfrecords\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_file_paths(folder_path):\n",
        "    \"\"\"\n",
        "    Get a list of file paths in the specified folder.\n",
        "    \"\"\"\n",
        "    file_paths = []\n",
        "    for root, dirs, files in os.walk(folder_path):\n",
        "        for file in files:\n",
        "            file_paths.append(os.path.join(root, file))\n",
        "    return np.array(file_paths)\n",
        "\n",
        "def save_file_paths(file_paths, output_file):\n",
        "    \"\"\"\n",
        "    Save the file paths to a NumPy file.\n",
        "    \"\"\"\n",
        "    np.save(output_file, file_paths)\n",
        "\n",
        "def main(folder_path, output_file):\n",
        "    file_paths = get_file_paths(folder_path)\n",
        "    save_file_paths(file_paths, output_file)\n",
        "    print(f\"File paths saved to {output_file}\")\n",
        "\n",
        "\n",
        "folder_path = '/content/drive/MyDrive/Thesis/Data/test_within_shuffled_pixels_with_index'\n",
        "output_file = '/content/drive/MyDrive/Thesis/Data/test_dataset_shuffled_pixels_index_final.npy'\n",
        "main(folder_path, output_file)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZZUy6jLVgZNb",
        "outputId": "ccfa7c50-63df-4f50-b856-77342b78edf1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "File paths saved to /content/drive/MyDrive/Thesis/Data/test_dataset_shuffled_pixels_index_final.npy\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Prepare Greenland dataset for Training\n",
        "#Define Helper Functions to encode TensorFlow compatible types into TFRecords.\n",
        "def _bytes_feature(value):\n",
        "    \"\"\"Returns a bytes_list from a string / byte.\"\"\"\n",
        "    return tf.train.Feature(bytes_list=tf.train.BytesList(value=[tf.io.serialize_tensor(value).numpy()]))\n",
        "\n",
        "def serialize_example(feature, label):\n",
        "    feature_dict = {\n",
        "        'feature': _bytes_feature(tf.convert_to_tensor(feature, dtype=tf.float32)),\n",
        "        'label': _bytes_feature(tf.convert_to_tensor(label, dtype=tf.int32))\n",
        "    }\n",
        "    example_proto = tf.train.Example(features=tf.train.Features(feature=feature_dict))\n",
        "    return example_proto.SerializeToString()\n",
        "\n",
        "\n",
        "# #Read TIFF Files\n",
        "def read_tiff_file(file_path):\n",
        "    with rasterio.open(file_path) as src:\n",
        "        data = src.read()  # Read all bands at once\n",
        "        features = np.transpose(data[:30, :, :], axes=(1, 2, 0))  # Transpose to height x width x channels\n",
        "        label = data[30, :, :]  # Last band: label\n",
        "        return features.astype(np.float32), label.astype(np.int32)\n",
        "\n",
        "\n",
        "# #Split data\n",
        "def stratified_split_data(file_paths, train_ratio=0.7, val_ratio=0.2, test_ratio=0.1):\n",
        "    draining_files = [fp for fp in file_paths if \"DrainingEvents\" in fp]\n",
        "    refreezing_files = [fp for fp in file_paths if \"RefreezingEvents\" in fp]\n",
        "\n",
        "    def split_files(files):\n",
        "        np.random.shuffle(files)\n",
        "        n = len(files)\n",
        "        return {\n",
        "            'train': files[:int(n * train_ratio)],\n",
        "            'val': files[int(n * train_ratio):int(n * (train_ratio + val_ratio))],\n",
        "            'test': files[int(n * (train_ratio + val_ratio)):]\n",
        "        }\n",
        "\n",
        "    return {\n",
        "        'train': split_files(draining_files)['train'] + split_files(refreezing_files)['train'],\n",
        "        'val': split_files(draining_files)['val'] + split_files(refreezing_files)['val'],\n",
        "        'test': split_files(draining_files)['test'] + split_files(refreezing_files)['test']\n",
        "    }\n",
        "\n",
        "#Write to TFRecord\n",
        "def write_tfrecord(file_name, file_paths):\n",
        "    with tf.io.TFRecordWriter(file_name) as writer:\n",
        "        for file_path in file_paths:\n",
        "            features, label = read_tiff_file(file_path)\n",
        "            example = serialize_example(features, label)\n",
        "            writer.write(example)\n",
        "        print(f\"Wrote {len(file_paths)} entries to {file_name}\")\n",
        "    return file_paths"
      ],
      "metadata": {
        "id": "PAUjnSHWuK39"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Implement the conversion\n",
        "file_paths = all_paths\n",
        "\n",
        "# Get stratified split\n",
        "splits = stratified_split_data(file_paths)\n",
        "\n",
        "# Print file names in the train dataset to check the balance\n",
        "print(\"Files in the training dataset:\")\n",
        "for f in splits['train']:\n",
        "    print(f)\n",
        "\n",
        "print(\"Files in the testing dataset:\")\n",
        "for f in splits['test']:\n",
        "    print(f)\n",
        "\n",
        "# Write TFRecords\n",
        "for split_name, paths in splits.items():\n",
        "    write_tfrecord(f'{split_name}.tfrecords', paths)\n",
        "    print(f'Wrote {len(paths)} records to {split_name}.tfrecords')\n",
        "\n",
        "# Write TFRecords for each split\n",
        "for split_name, paths in splits.items():\n",
        "    tfrecord_filename = f'/content/drive/MyDrive/Results/Datasets_int/{split_name}.tfrecords'\n",
        "    write_tfrecord(tfrecord_filename, paths)\n",
        "    print(f'Wrote {len(paths)} records to {tfrecord_filename}')"
      ],
      "metadata": {
        "id": "9scgSm8ceCdA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Chcek the Balance in Classes (refreezing, draining)\n",
        "def count_categories(files):\n",
        "    draining_count = len([f for f in files if \"DrainingEvents\" in f])\n",
        "    refreezing_count = len([f for f in files if \"RefreezingEvents\" in f])\n",
        "    return draining_count, refreezing_count\n",
        "\n",
        "train_counts = count_categories(splits['train'])\n",
        "val_counts = count_categories(splits['val'])\n",
        "test_counts = count_categories(splits['test'])\n",
        "\n",
        "print(\"Training set - Draining: {}, Refreezing: {}\".format(*train_counts))\n",
        "print(\"Validation set - Draining: {}, Refreezing: {}\".format(*val_counts))\n",
        "print(\"Testing set - Draining: {}, Refreezing: {}\".format(*test_counts))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tMs7qF5EW8EH",
        "outputId": "bf0f8330-022c-494c-b304-e0f01c4d8a7e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training set - Draining: 64, Refreezing: 65\n",
            "Validation set - Draining: 18, Refreezing: 18\n",
            "Testing set - Draining: 10, Refreezing: 10\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# file_paths = np.load('/content/drive/MyDrive/Thesis/Data/test_dataset_corrected_image_removed.npy', allow_pickle = True)\n",
        "tfrecord_filename = '/content/drive/MyDrive/Results/Datasets_test_various/test_dataset_shuffled_pixels_index.tfrecords'\n",
        "\n",
        "write_tfrecord(tfrecord_filename, file_paths)\n",
        "batch_size = 8\n",
        "\n",
        "#Load and use the data\n",
        "def _parse_function(proto):\n",
        "    feature_description = {\n",
        "        'feature': tf.io.FixedLenFeature([], tf.string),\n",
        "        'label': tf.io.FixedLenFeature([], tf.string),\n",
        "    }\n",
        "    parsed_features = tf.io.parse_single_example(proto, feature_description) #ok\n",
        "    feature = tf.io.parse_tensor(parsed_features['feature'], out_type=tf.float32)\n",
        "    label = tf.io.parse_tensor(parsed_features['label'],out_type=tf.int32)\n",
        "    feature.set_shape([64, 64, 30])\n",
        "    label.set_shape([64, 64])\n",
        "    return feature, label\n",
        "\n",
        "\n",
        "def load_dataset(tfrecord_file, batch_size):\n",
        "    dataset = tf.data.TFRecordDataset(tfrecord_file)\n",
        "    dataset = dataset.map(_parse_function)\n",
        "    dataset = dataset.batch(batch_size)\n",
        "    # dataset = dataset.shuffle(1000).batch(batch_size) #.repeat()\n",
        "    return dataset"
      ],
      "metadata": {
        "id": "i8iuPJiMeLDH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0b78d08a-3e00-4813-9f92-aedbea6dffcc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Wrote 19 entries to /content/drive/MyDrive/Results/Datasets_test_various/test_dataset_shuffled_pixels_index.tfrecords\n",
            "19\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = load_dataset(tfrecord_filename, batch_size)\n",
        "for features, labels in dataset.take(1):\n",
        "    print(\"Features batch shape:\", features.shape, features.dtype)  # Expected: (BATCH_SIZE, 64, 64, 30)\n",
        "    print(\"Labels batch shape:\", labels.shape, labels.dtype)      # Expected: (BATCH_SIZE, 64"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5FjlJJ1Pak7V",
        "outputId": "9a82854b-6b75-4cfa-d373-da6e604f0746"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Features batch shape: (8, 64, 64, 30) <dtype: 'float32'>\n",
            "Labels batch shape: (8, 64, 64) <dtype: 'int32'>\n"
          ]
        }
      ]
    }
  ]
}
